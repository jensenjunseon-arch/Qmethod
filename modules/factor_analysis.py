"""
Step 5: Statistical Analysis Module (í†µê³„ì  ë¶„ì„)
Factor Analysisë¥¼ ìˆ˜í–‰í•˜ì—¬ ìœ ì˜ë¯¸í•œ ìš”ì¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
"""
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from factor_analyzer import FactorAnalyzer
from factor_analyzer.factor_analyzer import calculate_kmo
import config


def perform_pca_analysis(df: pd.DataFrame) -> dict:
    """
    PCA ë¶„ì„ì„ ìˆ˜í–‰í•˜ì—¬ ì£¼ìš” ìš”ì¸ì„ ì‹ë³„í•©ë‹ˆë‹¤.
    
    Args:
        df: Q-Sorting ë°ì´í„° ë§¤íŠ¸ë¦­ìŠ¤ (ì°¸ì—¬ì x ë¬¸í•­)
    
    Returns:
        PCA ë¶„ì„ ê²°ê³¼
    """
    # ë°ì´í„° ì „ì¹˜ (Që°©ë²•ë¡ ì—ì„œëŠ” ì°¸ì—¬ìë¥¼ ë³€ìˆ˜ë¡œ, ë¬¸í•­ì„ ê´€ì¸¡ì¹˜ë¡œ ì²˜ë¦¬)
    data_transposed = df.T.values
    
    # â˜… ì¤‘ìš”: ìƒê´€í–‰ë ¬ ê¸°ë°˜ PCAë¥¼ ìœ„í•´ ë°ì´í„° í‘œì¤€í™” (Z-score)
    # ì´ë ‡ê²Œ í•´ì•¼ Eigenvalueê°€ ë³€ìˆ˜ ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°ë¨ (í•©ê³„ = ë³€ìˆ˜ ìˆ˜)
    scaler = StandardScaler()
    data_standardized = scaler.fit_transform(data_transposed)
    
    # PCA ìˆ˜í–‰ (ëª¨ë“  ì»´í¬ë„ŒíŠ¸)
    pca = PCA()
    pca.fit(data_standardized)
    
    # â˜… ìƒê´€í–‰ë ¬ ê¸°ë°˜ Eigenvalue ê³„ì‚°
    # í‘œì¤€í™”ëœ ë°ì´í„°ì—ì„œ explained_variance_ëŠ” ìƒê´€í–‰ë ¬ì˜ Eigenvalueì™€ ë™ì¼
    eigenvalues = pca.explained_variance_
    n_factors = sum(1 for ev in eigenvalues if ev >= config.EIGENVALUE_THRESHOLD)
    
    print(f"[PCA] ë°ì´í„° shape: {data_transposed.shape}", flush=True)
    print(f"[PCA] ì´ Eigenvalue í•©ê³„: {sum(eigenvalues):.2f} (ë³€ìˆ˜ ìˆ˜ì™€ ë™ì¼í•´ì•¼ í•¨)", flush=True)
    print(f"[PCA] Eigenvalue >= 1.0ì¸ ìš”ì¸ ìˆ˜: {n_factors}", flush=True)
    
    return {
        "eigenvalues": eigenvalues.tolist(),
        "explained_variance_ratio": pca.explained_variance_ratio_.tolist(),
        "cumulative_variance": np.cumsum(pca.explained_variance_ratio_).tolist(),
        "n_factors": max(n_factors, 2),  # ìµœì†Œ 2ê°œ ìš”ì¸
        "components": pca.components_
    }


def perform_factor_analysis(
    df: pd.DataFrame,
    n_factors: int = None,
    rotation: str = "varimax"
) -> dict:
    """
    ìš”ì¸ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    scikit-learn í˜¸í™˜ì„±ì„ ìœ„í•´ PCA ì§ì ‘ ì‚¬ìš©
    
    Args:
        df: Q-Sorting ë°ì´í„° ë§¤íŠ¸ë¦­ìŠ¤
        n_factors: ì¶”ì¶œí•  ìš”ì¸ ìˆ˜ (Noneì´ë©´ PCAë¡œ ê²°ì •)
        rotation: íšŒì „ ë°©ë²• (varimax)
    
    Returns:
        ìš”ì¸ ë¶„ì„ ê²°ê³¼
    """
    print("\n" + "="*60, flush=True)
    print("ğŸ“ˆ í†µê³„ì  ë¶„ì„ (Factor Analysis)", flush=True)
    print("="*60, flush=True)
    
    # ë°ì´í„° ì „ì¹˜
    data_transposed = df.T.values
    
    # â˜… ìƒê´€í–‰ë ¬ ê¸°ë°˜ ë¶„ì„ì„ ìœ„í•´ ë°ì´í„° í‘œì¤€í™”
    scaler = StandardScaler()
    data_standardized = scaler.fit_transform(data_transposed)
    
    # PCAë¡œ ìš”ì¸ ìˆ˜ ê²°ì •
    if n_factors is None:
        pca_result = perform_pca_analysis(df)
        n_factors = pca_result["n_factors"]
        eigenvalues = pca_result["eigenvalues"]
        print(f"\nğŸ”¢ Eigenvalue > 1.0 ê¸°ì¤€ ìš”ì¸ ìˆ˜: {n_factors}", flush=True)
        print(f"   Eigenvalues: {[f'{ev:.2f}' for ev in eigenvalues[:n_factors+2]]}", flush=True)
    
    # PCA ì§ì ‘ ìˆ˜í–‰ (factor_analyzer ëŒ€ì‹ )
    try:
        # ë¨¼ì € factor_analyzer ì‹œë„ (í‘œì¤€í™”ëœ ë°ì´í„° ì‚¬ìš©)
        fa = FactorAnalyzer(n_factors=n_factors, rotation=rotation, method='principal')
        fa.fit(data_standardized)  # â˜… í‘œì¤€í™”ëœ ë°ì´í„° ì‚¬ìš©
        loadings = fa.loadings_
        variance = fa.get_factor_variance()
        ss_loadings = variance[0].tolist()
        proportion_var = variance[1].tolist()
        cumulative_var = variance[2].tolist()
    except Exception as e:
        print(f"âš ï¸ factor_analyzer ì˜¤ë¥˜, PCAë¡œ ëŒ€ì²´: {e}", flush=True)
        # PCA ì§ì ‘ ì‚¬ìš© (í‘œì¤€í™”ëœ ë°ì´í„°)
        pca = PCA(n_components=n_factors)
        pca.fit(data_standardized)  # â˜… í‘œì¤€í™”ëœ ë°ì´í„° ì‚¬ìš©
        loadings = pca.components_.T  # Transpose to get (n_features, n_components)
        
        # Varimax íšŒì „ ìˆ˜ë™ ì ìš©
        if rotation == "varimax":
            loadings = varimax_rotation(loadings)
        
        # ë¶„ì‚° ê³„ì‚° (ìƒê´€í–‰ë ¬ ê¸°ë°˜ PCAì—ì„œëŠ” ì´ ë¶„ì‚° = ë³€ìˆ˜ ìˆ˜)
        n_vars = data_standardized.shape[1]  # ë³€ìˆ˜(ì°¸ì—¬ì) ìˆ˜
        # PCAì˜ explained_variance_ratio_ ì‚¬ìš©
        proportion_var = list(pca.explained_variance_ratio_)
        ss_loadings = list(pca.explained_variance_)  # Eigenvalues
        cumulative_var = np.cumsum(proportion_var).tolist()
    
    # ê° ì°¸ì—¬ìì˜ ìš”ì¸ ì ìˆ˜ ê³„ì‚°
    factor_scores = calculate_factor_scores(df, loadings)
    
    # ìœ ì˜ë¯¸í•œ ì ì¬ëŸ‰ì„ ê°€ì§„ ì°¸ì—¬ì ì‹ë³„
    significant_loadings = identify_significant_loadings(
        loadings, 
        df.index.tolist(),
        threshold=config.MIN_FACTOR_LOADING
    )
    
    print(f"\nğŸ“Š ë¶„ì„ ê²°ê³¼:", flush=True)
    print(f"   ì¶”ì¶œëœ ìš”ì¸ ìˆ˜: {n_factors}", flush=True)
    print(f"   ì´ ì„¤ëª… ë¶„ì‚°: {sum(proportion_var):.2%}", flush=True)
    
    for i in range(n_factors):
        print(f"   - Factor {i+1}: {proportion_var[i]:.2%} (SS Loading: {ss_loadings[i]:.2f})", flush=True)
    
    return {
        "n_factors": n_factors,
        "eigenvalues": eigenvalues,  # â˜… ì‹¤ì œ Eigenvalue ì¶”ê°€
        "loadings": loadings,
        "loadings_df": pd.DataFrame(
            loadings,
            index=df.index,
            columns=[f"Factor{i+1}" for i in range(n_factors)]
        ),
        "variance": {
            "ss_loadings": ss_loadings,
            "proportion_var": proportion_var,
            "cumulative_var": cumulative_var
        },
        "factor_scores": factor_scores,
        "significant_loadings": significant_loadings
    }


def varimax_rotation(loadings: np.ndarray, max_iter: int = 100, tol: float = 1e-5) -> np.ndarray:
    """
    Varimax íšŒì „ ìˆ˜ë™ êµ¬í˜„
    """
    n_vars, n_factors = loadings.shape
    rotation_matrix = np.eye(n_factors)
    
    for _ in range(max_iter):
        old_rotation = rotation_matrix.copy()
        
        for i in range(n_factors):
            for j in range(i + 1, n_factors):
                # Varimax criterion
                x = loadings[:, i]
                y = loadings[:, j]
                
                u = x**2 - y**2
                v = 2 * x * y
                
                A = np.sum(u)
                B = np.sum(v)
                C = np.sum(u**2 - v**2)
                D = np.sum(2 * u * v)
                
                num = D - 2 * A * B / n_vars
                den = C - (A**2 - B**2) / n_vars
                
                phi = 0.25 * np.arctan2(num, den)
                
                # Rotation
                cos_phi = np.cos(phi)
                sin_phi = np.sin(phi)
                
                loadings[:, i] = x * cos_phi + y * sin_phi
                loadings[:, j] = -x * sin_phi + y * cos_phi
        
        # Check convergence
        if np.allclose(rotation_matrix, old_rotation, atol=tol):
            break
    
    return loadings



def calculate_factor_scores(df: pd.DataFrame, loadings: np.ndarray) -> pd.DataFrame:
    """
    ê° ë¬¸í•­ì˜ ìš”ì¸ë³„ Z-scoreë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        df: Q-Sorting ë°ì´í„° ë§¤íŠ¸ë¦­ìŠ¤
        loadings: ìš”ì¸ ì ì¬ëŸ‰ ë§¤íŠ¸ë¦­ìŠ¤
    
    Returns:
        ìš”ì¸ë³„ ë¬¸í•­ Z-score DataFrame
    """
    n_factors = loadings.shape[1]
    n_items = df.shape[1]
    
    factor_scores = np.zeros((n_items, n_factors))
    
    for factor_idx in range(n_factors):
        # í•´ë‹¹ ìš”ì¸ì— ìœ ì˜ë¯¸í•˜ê²Œ ì ì¬ëœ ì°¸ì—¬ìë“¤ì˜ ê°€ì¤‘ í‰ê· 
        factor_loadings = loadings[:, factor_idx]
        
        # ìœ ì˜ë¯¸í•œ ì ì¬ëŸ‰ì„ ê°€ì§„ ì°¸ì—¬ìë§Œ ì„ íƒ
        significant_mask = np.abs(factor_loadings) >= config.MIN_FACTOR_LOADING
        
        # â˜… ìœ ì˜ë¯¸í•œ ì°¸ì—¬ìê°€ ì—†ìœ¼ë©´, ìƒìœ„ 3ëª…ì´ë¼ë„ ì‚¬ìš©
        if not np.any(significant_mask):
            # ì ì¬ëŸ‰ ì ˆëŒ€ê°’ ê¸°ì¤€ ìƒìœ„ 3ëª… ì„ íƒ
            top_indices = np.argsort(np.abs(factor_loadings))[-3:]
            significant_mask = np.zeros(len(factor_loadings), dtype=bool)
            significant_mask[top_indices] = True
        
        weights = factor_loadings[significant_mask]
        weighted_data = df.iloc[significant_mask].values
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        if len(weights) > 0 and np.sum(np.abs(weights)) > 0:
            weighted_sum = np.average(weighted_data, axis=0, weights=np.abs(weights))
            
            # Z-score ë³€í™˜ (std=0 ë°©ì§€)
            std_val = np.std(weighted_sum)
            if std_val > 0:
                factor_scores[:, factor_idx] = (weighted_sum - np.mean(weighted_sum)) / std_val
            else:
                # í‘œì¤€í¸ì°¨ 0ì´ë©´ ëª¨ë“  ê°’ì´ ë™ì¼ â†’ ì¤‘ì•™í™”ë§Œ
                factor_scores[:, factor_idx] = weighted_sum - np.mean(weighted_sum)
    
    return pd.DataFrame(
        factor_scores,
        index=[f"Q{i+1}" for i in range(n_items)],
        columns=[f"Factor{i+1}" for i in range(n_factors)]
    )


def identify_significant_loadings(
    loadings: np.ndarray,
    participant_names: list[str],
    threshold: float = 0.4
) -> dict:
    """
    ìœ ì˜ë¯¸í•œ ìš”ì¸ ì ì¬ëŸ‰ì„ ê°€ì§„ ì°¸ì—¬ìë¥¼ ì‹ë³„í•©ë‹ˆë‹¤.
    
    Args:
        loadings: ìš”ì¸ ì ì¬ëŸ‰ ë§¤íŠ¸ë¦­ìŠ¤
        participant_names: ì°¸ì—¬ì ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        threshold: ìœ ì˜ë¯¸í•œ ì ì¬ëŸ‰ ì„ê³„ê°’
    
    Returns:
        ìš”ì¸ë³„ ìœ ì˜ë¯¸í•œ ì°¸ì—¬ì ë”•ì…”ë„ˆë¦¬
    """
    n_factors = loadings.shape[1]
    result = {}
    
    for factor_idx in range(n_factors):
        factor_name = f"Factor{factor_idx + 1}"
        factor_loadings = loadings[:, factor_idx]
        
        significant = []
        for i, (name, loading) in enumerate(zip(participant_names, factor_loadings)):
            if abs(loading) >= threshold:
                significant.append({
                    "name": name,
                    "loading": loading,
                    "direction": "positive" if loading > 0 else "negative"
                })
        
        # ì ì¬ëŸ‰ í¬ê¸°ë¡œ ì •ë ¬
        significant.sort(key=lambda x: abs(x["loading"]), reverse=True)
        result[factor_name] = significant
    
    return result


def identify_consensus_statements(
    factor_scores: pd.DataFrame,
    q_set: list[str],
    threshold: float = 0.5
) -> list[dict]:
    """
    í•©ì˜ ë¬¸í•­(Consensus Statements)ì„ ì‹ë³„í•©ë‹ˆë‹¤.
    ëª¨ë“  Factorì—ì„œ ë¹„ìŠ·í•œ Z-scoreë¥¼ ë°›ì€ ë¬¸í•­ë“¤ì…ë‹ˆë‹¤.
    
    Args:
        factor_scores: ìš”ì¸ë³„ ë¬¸í•­ Z-score DataFrame
        q_set: Q-Set ë¬¸í•­ ë¦¬ìŠ¤íŠ¸
        threshold: Z-score ì°¨ì´ ì„ê³„ê°’ (ì´ ì´í•˜ë©´ í•©ì˜ë¡œ íŒë‹¨)
    
    Returns:
        í•©ì˜ ë¬¸í•­ ë¦¬ìŠ¤íŠ¸
    """
    consensus = []
    n_factors = len(factor_scores.columns)
    
    for idx in factor_scores.index:
        item_num = int(idx.replace("Q", "")) - 1
        scores = factor_scores.loc[idx].values
        
        # ëª¨ë“  Factor ê°„ Z-score ì°¨ì´ ê³„ì‚°
        max_diff = max(scores) - min(scores)
        avg_score = np.mean(scores)
        
        # ì°¨ì´ê°€ ì„ê³„ê°’ ì´í•˜ë©´ í•©ì˜ ë¬¸í•­
        if max_diff <= threshold:
            consensus.append({
                "item_number": item_num + 1,
                "statement": q_set[item_num] if item_num < len(q_set) else f"Q{item_num+1}",
                "avg_z_score": float(avg_score),
                "max_difference": float(max_diff),
                "factor_scores": {col: float(factor_scores.loc[idx, col]) for col in factor_scores.columns}
            })
    
    # í‰ê·  Z-score ì ˆëŒ€ê°’ìœ¼ë¡œ ì •ë ¬ (ê°•í•œ í•©ì˜ê°€ ë¨¼ì €)
    consensus.sort(key=lambda x: abs(x["avg_z_score"]), reverse=True)
    
    return consensus


def identify_distinguishing_statements(
    factor_scores: pd.DataFrame,
    q_set: list[str],
    threshold: float = 1.0
) -> dict:
    """
    êµ¬ë¶„ ë¬¸í•­(Distinguishing Statements)ì„ ì‹ë³„í•©ë‹ˆë‹¤.
    íŠ¹ì • Factorì—ì„œë§Œ ë†’ê±°ë‚˜ ë‚®ì€ Z-scoreë¥¼ ë³´ì´ëŠ” ë¬¸í•­ë“¤ì…ë‹ˆë‹¤.
    
    Args:
        factor_scores: ìš”ì¸ë³„ ë¬¸í•­ Z-score DataFrame
        q_set: Q-Set ë¬¸í•­ ë¦¬ìŠ¤íŠ¸
        threshold: ë‹¤ë¥¸ Factorì™€ì˜ Z-score ì°¨ì´ ì„ê³„ê°’
    
    Returns:
        Factorë³„ êµ¬ë¶„ ë¬¸í•­ ë”•ì…”ë„ˆë¦¬
    """
    distinguishing = {}
    
    for col in factor_scores.columns:
        other_cols = [c for c in factor_scores.columns if c != col]
        dist_items = []
        
        for idx in factor_scores.index:
            item_num = int(idx.replace("Q", "")) - 1
            this_score = factor_scores.loc[idx, col]
            other_scores = [factor_scores.loc[idx, c] for c in other_cols]
            
            # ë‹¤ë¥¸ ëª¨ë“  Factorë³´ë‹¤ í˜„ì €íˆ ë†’ê±°ë‚˜ ë‚®ì€ ê²½ìš°
            min_diff = min([abs(this_score - other) for other in other_scores])
            
            if min_diff >= threshold:
                dist_items.append({
                    "item_number": item_num + 1,
                    "statement": q_set[item_num] if item_num < len(q_set) else f"Q{item_num+1}",
                    "z_score": float(this_score),
                    "min_diff_from_others": float(min_diff),
                    "direction": "high" if this_score > 0 else "low"
                })
        
        # Z-score ì°¨ì´ë¡œ ì •ë ¬
        dist_items.sort(key=lambda x: x["min_diff_from_others"], reverse=True)
        distinguishing[col] = dist_items
    
    return distinguishing


def get_factor_interpretation_data(
    factor_scores: pd.DataFrame,
    q_set: list[str],
    top_n: int = 5
) -> dict:
    """
    ê° ìš”ì¸ í•´ì„ì„ ìœ„í•œ ë°ì´í„°ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤.
    
    Args:
        factor_scores: ìš”ì¸ë³„ ë¬¸í•­ Z-score
        q_set: Q-Set ë¬¸í•­ ë¦¬ìŠ¤íŠ¸
        top_n: ìƒìœ„/í•˜ìœ„ ë¬¸í•­ ìˆ˜
    
    Returns:
        ìš”ì¸ë³„ í•´ì„ ë°ì´í„°
    """
    result = {}
    
    for col in factor_scores.columns:
        factor_scores_sorted = factor_scores[col].sort_values(ascending=False)
        
        # ìƒìœ„ ë¬¸í•­ (ê°€ì¥ ë™ì˜)
        top_items = []
        for idx in factor_scores_sorted.head(top_n).index:
            item_num = int(idx.replace("Q", "")) - 1
            top_items.append({
                "item_number": item_num + 1,
                "statement": q_set[item_num],
                "z_score": factor_scores_sorted[idx]
            })
        
        # í•˜ìœ„ ë¬¸í•­ (ê°€ì¥ ë¹„ë™ì˜)
        bottom_items = []
        for idx in factor_scores_sorted.tail(top_n).index:
            item_num = int(idx.replace("Q", "")) - 1
            bottom_items.append({
                "item_number": item_num + 1,
                "statement": q_set[item_num],
                "z_score": factor_scores_sorted[idx]
            })
        
        result[col] = {
            "top_items": top_items,
            "bottom_items": bottom_items[::-1],  # ê°€ì¥ ë‚®ì€ ê²ƒë¶€í„°
            "mean_score": factor_scores[col].mean(),
            "std_score": factor_scores[col].std()
        }
    
    return result


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ë°ì´í„°
    np.random.seed(42)
    dummy_data = pd.DataFrame(
        np.random.randint(-5, 6, size=(20, 60)),
        index=[f"P{i+1}" for i in range(20)],
        columns=[f"Q{i+1}" for i in range(60)]
    )
    
    result = perform_factor_analysis(dummy_data)
    print("\nìš”ì¸ ì ì¬ëŸ‰:")
    print(result["loadings_df"])
